{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 26.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found locally - downloading via kagglehub...\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.3).\n",
      "Using dataset directory: /Users/glennc/.cache/kagglehub/datasets/computingvictor/transactions-fraud-datasets/versions/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10649266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23410063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9316588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12478022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9558530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id  target\n",
       "0       10649266       0\n",
       "1       23410063       0\n",
       "2        9316588       0\n",
       "3       12478022       0\n",
       "4        9558530       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "DATASET_ID = \"computingvictor/transactions-fraud-datasets\"\n",
    "DATASET_FOLDER_NAME = \"transactions-fraud-datasets\"\n",
    "\n",
    "def find_repo_root():\n",
    "    cur = Path.cwd().resolve()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "def resolve_dataset_dir():\n",
    "    # 1) Environment variable override\n",
    "    env = os.getenv(\"FRAUD_DATA_DIR\")\n",
    "    if env:\n",
    "        p = Path(env).expanduser().resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # 2) Repo-relative data folder\n",
    "    repo_root = find_repo_root()\n",
    "    local = repo_root / \"data\" / DATASET_FOLDER_NAME\n",
    "    if local.exists():\n",
    "        return local.resolve()\n",
    "\n",
    "    # 3) Auto-download via kagglehub\n",
    "    print(\"Dataset not found locally - downloading via kagglehub...\")\n",
    "    return Path(kagglehub.dataset_download(DATASET_ID)).resolve()\n",
    "\n",
    "dataset_dir = resolve_dataset_dir()\n",
    "print(\"Using dataset directory:\", dataset_dir)\n",
    "\n",
    "tx_path = dataset_dir / \"transactions_data.csv\"\n",
    "labels_path = dataset_dir / \"train_fraud_labels.json\"\n",
    "\n",
    "assert tx_path.exists(), f\"Missing {tx_path}\"\n",
    "assert labels_path.exists(), f\"Missing {labels_path}\"\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels_raw = json.load(f)\n",
    "\n",
    "target_map = labels_raw[\"target\"]  # dict: transaction_id -> \"Yes\"/\"No\"\n",
    "\n",
    "labels = pd.DataFrame({\n",
    "    \"transaction_id\": list(target_map.keys()),\n",
    "    \"target\": [1 if v == \"Yes\" else 0 for v in target_map.values()]\n",
    "})\n",
    "\n",
    "labels[\"transaction_id\"] = labels[\"transaction_id\"].astype(str)\n",
    "labels[\"target\"] = labels[\"target\"].astype(int)\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset transactions: (3358392, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>client_id</th>\n",
       "      <th>card_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>use_chip</th>\n",
       "      <th>merchant_state</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7475328</td>\n",
       "      <td>2010-01-01 00:02:00</td>\n",
       "      <td>561</td>\n",
       "      <td>4575</td>\n",
       "      <td>$14.57</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>IA</td>\n",
       "      <td>5311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7475329</td>\n",
       "      <td>2010-01-01 00:02:00</td>\n",
       "      <td>1129</td>\n",
       "      <td>102</td>\n",
       "      <td>$80.00</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>CA</td>\n",
       "      <td>4829</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7475333</td>\n",
       "      <td>2010-01-01 00:07:00</td>\n",
       "      <td>1807</td>\n",
       "      <td>165</td>\n",
       "      <td>$4.81</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>NY</td>\n",
       "      <td>5942</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7475337</td>\n",
       "      <td>2010-01-01 00:21:00</td>\n",
       "      <td>351</td>\n",
       "      <td>1112</td>\n",
       "      <td>$10.74</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>NY</td>\n",
       "      <td>5813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7475344</td>\n",
       "      <td>2010-01-01 00:32:00</td>\n",
       "      <td>646</td>\n",
       "      <td>2093</td>\n",
       "      <td>$73.79</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>PA</td>\n",
       "      <td>7538</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                 date  client_id  card_id  amount  \\\n",
       "0  7475328  2010-01-01 00:02:00        561     4575  $14.57   \n",
       "1  7475329  2010-01-01 00:02:00       1129      102  $80.00   \n",
       "2  7475333  2010-01-01 00:07:00       1807      165   $4.81   \n",
       "3  7475337  2010-01-01 00:21:00        351     1112  $10.74   \n",
       "4  7475344  2010-01-01 00:32:00        646     2093  $73.79   \n",
       "\n",
       "            use_chip merchant_state   mcc errors  \n",
       "0  Swipe Transaction             IA  5311    NaN  \n",
       "1  Swipe Transaction             CA  4829    NaN  \n",
       "2  Swipe Transaction             NY  5942    NaN  \n",
       "3  Swipe Transaction             NY  5813    NaN  \n",
       "4  Swipe Transaction             PA  7538    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "usecols = [\n",
    "    \"id\", \"date\", \"client_id\", \"card_id\", \"amount\",\n",
    "    \"use_chip\", \"merchant_state\", \"mcc\", \"errors\"\n",
    "]\n",
    "\n",
    "# Choose clients from an early slice of the dataset\n",
    "first_chunk = pd.read_csv(tx_path, usecols=[\"client_id\"], nrows=200_000)\n",
    "clients = first_chunk[\"client_id\"].dropna().unique()\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "N_CLIENTS = 300   \n",
    "chosen_clients = set(np.random.choice(clients, size=min(N_CLIENTS, len(clients)), replace=False))\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(tx_path, usecols=usecols, chunksize=200_000):\n",
    "    keep = chunk[chunk[\"client_id\"].isin(chosen_clients)]\n",
    "    if len(keep):\n",
    "        chunks.append(keep)\n",
    "\n",
    "tx = pd.concat(chunks, ignore_index=True)\n",
    "tx[\"id\"] = tx[\"id\"].astype(str)\n",
    "\n",
    "print(\"Subset transactions:\", tx.shape)\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 3358392\n",
      "Labeled rows: 2249957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>client_id</th>\n",
       "      <th>card_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>use_chip</th>\n",
       "      <th>merchant_state</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7475328</td>\n",
       "      <td>2010-01-01 00:02:00</td>\n",
       "      <td>561</td>\n",
       "      <td>4575</td>\n",
       "      <td>$14.57</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>IA</td>\n",
       "      <td>5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7475329</td>\n",
       "      <td>2010-01-01 00:02:00</td>\n",
       "      <td>1129</td>\n",
       "      <td>102</td>\n",
       "      <td>$80.00</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>CA</td>\n",
       "      <td>4829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7475333</td>\n",
       "      <td>2010-01-01 00:07:00</td>\n",
       "      <td>1807</td>\n",
       "      <td>165</td>\n",
       "      <td>$4.81</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>NY</td>\n",
       "      <td>5942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7475337</td>\n",
       "      <td>2010-01-01 00:21:00</td>\n",
       "      <td>351</td>\n",
       "      <td>1112</td>\n",
       "      <td>$10.74</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>NY</td>\n",
       "      <td>5813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7475344</td>\n",
       "      <td>2010-01-01 00:32:00</td>\n",
       "      <td>646</td>\n",
       "      <td>2093</td>\n",
       "      <td>$73.79</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>PA</td>\n",
       "      <td>7538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                 date  client_id  card_id  amount  \\\n",
       "0  7475328  2010-01-01 00:02:00        561     4575  $14.57   \n",
       "1  7475329  2010-01-01 00:02:00       1129      102  $80.00   \n",
       "2  7475333  2010-01-01 00:07:00       1807      165   $4.81   \n",
       "3  7475337  2010-01-01 00:21:00        351     1112  $10.74   \n",
       "4  7475344  2010-01-01 00:32:00        646     2093  $73.79   \n",
       "\n",
       "            use_chip merchant_state   mcc errors  target  \n",
       "0  Swipe Transaction             IA  5311    NaN     0.0  \n",
       "1  Swipe Transaction             CA  4829    NaN     0.0  \n",
       "2  Swipe Transaction             NY  5942    NaN     0.0  \n",
       "3  Swipe Transaction             NY  5813    NaN     NaN  \n",
       "4  Swipe Transaction             PA  7538    NaN     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx[\"id\"] = tx[\"id\"].astype(str)\n",
    "\n",
    "tx = tx.merge(labels, left_on=\"id\", right_on=\"transaction_id\", how=\"left\")\n",
    "tx = tx.drop(columns=[\"transaction_id\"])\n",
    "\n",
    "print(\"Rows:\", len(tx))\n",
    "print(\"Labeled rows:\", tx[\"target\"].notna().sum())\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>client_id</th>\n",
       "      <th>card_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>use_chip</th>\n",
       "      <th>merchant_state</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7477094</td>\n",
       "      <td>2010-01-01 11:58:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>15.09</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>FL</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7477168</td>\n",
       "      <td>2010-01-01 12:11:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3682</td>\n",
       "      <td>6.01</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>FL</td>\n",
       "      <td>5813</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7477216</td>\n",
       "      <td>2010-01-01 12:18:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3682</td>\n",
       "      <td>14.58</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7477978</td>\n",
       "      <td>2010-01-01 15:09:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>14.66</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7478279</td>\n",
       "      <td>2010-01-01 16:26:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>22.77</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>FL</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                date  client_id  card_id  amount  \\\n",
       "0  7477094 2010-01-01 11:58:00          1     4652   15.09   \n",
       "1  7477168 2010-01-01 12:11:00          1     3682    6.01   \n",
       "2  7477216 2010-01-01 12:18:00          1     3682   14.58   \n",
       "3  7477978 2010-01-01 15:09:00          1     4652   14.66   \n",
       "4  7478279 2010-01-01 16:26:00          1     4652   22.77   \n",
       "\n",
       "             use_chip merchant_state   mcc errors  target  \n",
       "0   Swipe Transaction             FL  4121   None     0.0  \n",
       "1   Swipe Transaction             FL  5813   None     0.0  \n",
       "2  Online Transaction        Unknown  4121   None     0.0  \n",
       "3  Online Transaction        Unknown  4121   None     0.0  \n",
       "4   Swipe Transaction             FL  4121   None     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx[\"date\"] = pd.to_datetime(tx[\"date\"], errors=\"coerce\")\n",
    "tx = tx.dropna(subset=[\"date\"])\n",
    "\n",
    "def parse_amount(x):\n",
    "    x = str(x).replace(\"$\",\"\").replace(\",\",\"\").strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "tx[\"amount\"] = tx[\"amount\"].apply(parse_amount)\n",
    "tx = tx.dropna(subset=[\"amount\"])\n",
    "\n",
    "# Fill missing categoricals\n",
    "tx[\"errors\"] = tx[\"errors\"].fillna(\"None\").astype(str)\n",
    "tx[\"use_chip\"] = tx[\"use_chip\"].fillna(\"Unknown\").astype(str)\n",
    "tx[\"merchant_state\"] = tx[\"merchant_state\"].fillna(\"Unknown\").astype(str)\n",
    "tx[\"mcc\"] = tx[\"mcc\"].fillna(-1).astype(int).astype(str)  # treat as categorical\n",
    "\n",
    "# Sort per client\n",
    "tx = tx.sort_values([\"client_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2350739, 503758, 503895)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_split(df, client_col=\"client_id\", frac_train=0.70, frac_val=0.15, min_len=20):\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for cid, g in df.groupby(client_col, sort=False):\n",
    "        n = len(g)\n",
    "        if n < min_len:\n",
    "            continue\n",
    "        t1 = int(n * frac_train)\n",
    "        t2 = int(n * (frac_train + frac_val))\n",
    "        idx = g.index.to_numpy()\n",
    "        train_idx.append(idx[:t1])\n",
    "        val_idx.append(idx[t1:t2])\n",
    "        test_idx.append(idx[t2:])\n",
    "    return np.concatenate(train_idx), np.concatenate(val_idx), np.concatenate(test_idx)\n",
    "\n",
    "train_idx, val_idx, test_idx = time_split(tx)\n",
    "\n",
    "train_df = tx.loc[train_idx].copy()\n",
    "val_df   = tx.loc[val_idx].copy()\n",
    "test_df  = tx.loc[test_idx].copy()\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2350739, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "for d in [train_df, val_df, test_df]:\n",
    "    d[\"hour\"] = d[\"date\"].dt.hour\n",
    "    d[\"dayofweek\"] = d[\"date\"].dt.dayofweek\n",
    "\n",
    "num_cols = [\"amount\", \"hour\", \"dayofweek\"]\n",
    "cat_cols = [\"use_chip\", \"merchant_state\", \"mcc\", \"errors\"]\n",
    "\n",
    "enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Xtr_cat = enc.fit_transform(train_df[cat_cols])\n",
    "Xva_cat = enc.transform(val_df[cat_cols])\n",
    "Xte_cat = enc.transform(test_df[cat_cols])\n",
    "\n",
    "Xtr_num = scaler.fit_transform(train_df[num_cols])\n",
    "Xva_num = scaler.transform(val_df[num_cols])\n",
    "Xte_num = scaler.transform(test_df[num_cols])\n",
    "\n",
    "X_train = np.hstack([Xtr_num, Xtr_cat]).astype(np.float32)\n",
    "X_val   = np.hstack([Xva_num, Xva_cat]).astype(np.float32)\n",
    "X_test  = np.hstack([Xte_num, Xte_cat]).astype(np.float32)\n",
    "\n",
    "y_train = train_df[\"target\"].to_numpy()\n",
    "y_val   = val_df[\"target\"].to_numpy()\n",
    "y_test  = test_df[\"target\"].to_numpy()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 10, 7), np.float64(0.0018))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_sequences_sampled(df_part, X_part, y_part, seq_len=10, max_windows=50_000, client_col=\"client_id\"):\n",
    "    X_seqs = np.zeros((max_windows, seq_len, X_part.shape[1]), dtype=np.float32)\n",
    "    y_seqs = np.zeros((max_windows,), dtype=np.int64)\n",
    "\n",
    "    k = 0\n",
    "    start = 0\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    for cid, g in df_part.groupby(client_col, sort=False):\n",
    "        n = len(g)\n",
    "        if n <= seq_len:\n",
    "            start += n\n",
    "            continue\n",
    "\n",
    "        Xg = X_part[start:start+n]\n",
    "        yg = y_part[start:start+n]\n",
    "\n",
    "        # indices where label exists and we have enough history\n",
    "        valid_t = np.where(~np.isnan(yg))[0]\n",
    "        valid_t = valid_t[valid_t >= seq_len]\n",
    "        if len(valid_t) == 0:\n",
    "            start += n\n",
    "            continue\n",
    "\n",
    "        # sample up to some per client (keeps balance and speed)\n",
    "        take = min(len(valid_t), 50)\n",
    "        chosen = rng.choice(valid_t, size=take, replace=False)\n",
    "\n",
    "        for t in chosen:\n",
    "            if k >= max_windows:\n",
    "                return X_seqs[:k], y_seqs[:k]\n",
    "            X_seqs[k] = Xg[t-seq_len:t]\n",
    "            y_seqs[k] = int(yg[t])\n",
    "            k += 1\n",
    "\n",
    "        start += n\n",
    "\n",
    "    return X_seqs[:k], y_seqs[:k]\n",
    "\n",
    "\n",
    "SEQ_LEN = 10\n",
    "Xtr_seq, ytr_seq = build_sequences_sampled(train_df, X_train, y_train, seq_len=SEQ_LEN, max_windows=50_000)\n",
    "Xva_seq, yva_seq = build_sequences_sampled(val_df, X_val, y_val, seq_len=SEQ_LEN, max_windows=20_000)\n",
    "Xte_seq, yte_seq = build_sequences_sampled(test_df,  X_test,  y_test,  seq_len=SEQ_LEN)\n",
    "\n",
    "Xtr_seq.shape, ytr_seq.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights + intercept\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/glennc/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg best threshold (val): 0.99 | val metrics: {'precision': 0.002398081534772182, 'recall': 0.06666666666666667, 'f1': 0.004629629629629629, 'fpr': np.float64(0.027761094427761094), 'roc_auc': np.float64(0.5006161717272828), 'pr_auc': np.float64(0.0012614757612957484)}\n",
      "RF best threshold (val): 0.01 | val metrics: {'precision': 0.000945179584120983, 'recall': 0.06666666666666667, 'f1': 0.001863932898415657, 'fpr': np.float64(0.0705372038705372), 'roc_auc': np.float64(0.5373462351240129), 'pr_auc': np.float64(0.0011903203970417913)}\n"
     ]
    }
   ],
   "source": [
    "# Non-Neural Baseline Models (for comparison with LSTM)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "# Flatten LSTM windows for sklearn baselines: (N, seq_len, feat) -> (N, seq_len*feat)\n",
    "Xtr_flat = Xtr_seq.reshape(Xtr_seq.shape[0], -1)\n",
    "Xva_flat = Xva_seq.reshape(Xva_seq.shape[0], -1)\n",
    "Xte_flat = Xte_seq.reshape(Xte_seq.shape[0], -1)\n",
    "\n",
    "ytr = ytr_seq.astype(np.int64)\n",
    "yva = yva_seq.astype(np.int64)\n",
    "yte = yte_seq.astype(np.int64)\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    fpr = ((y_pred == 1) & (y_true == 0)).sum() / max((y_true == 0).sum(), 1)\n",
    "\n",
    "    roc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    pr  = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"fpr\": fpr, \"roc_auc\": roc, \"pr_auc\": pr}\n",
    "\n",
    "def best_threshold_f1(y_true, y_prob, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    best_t, best_f1, best_m = 0.5, -1, None\n",
    "    for t in thresholds:\n",
    "        m = compute_metrics(y_true, y_prob, threshold=t)\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1, best_t, best_m = m[\"f1\"], t, m\n",
    "    return best_t, best_m\n",
    "\n",
    "# --- Train baseline models (store probs so we can compare later) ---\n",
    "baseline_results = {}\n",
    "\n",
    "# Logistic Regression baseline\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\n",
    "logreg.fit(Xtr_flat, ytr)\n",
    "\n",
    "va_prob_lr = logreg.predict_proba(Xva_flat)[:, 1]\n",
    "best_t_lr, va_metrics_lr = best_threshold_f1(yva, va_prob_lr)\n",
    "te_prob_lr = logreg.predict_proba(Xte_flat)[:, 1]\n",
    "te_metrics_lr = compute_metrics(yte, te_prob_lr, threshold=best_t_lr)\n",
    "\n",
    "baseline_results[\"LogReg\"] = {\n",
    "    \"val_prob\": va_prob_lr, \"test_prob\": te_prob_lr,\n",
    "    \"best_t\": best_t_lr,\n",
    "    \"val\": va_metrics_lr, \"test\": te_metrics_lr\n",
    "}\n",
    "\n",
    "print(\"LogReg best threshold (val):\", best_t_lr, \"| val metrics:\", va_metrics_lr)\n",
    "\n",
    "# Random Forest baseline (optional; can be slower)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1, random_state=0\n",
    ")\n",
    "rf.fit(Xtr_flat, ytr)\n",
    "\n",
    "va_prob_rf = rf.predict_proba(Xva_flat)[:, 1]\n",
    "best_t_rf, va_metrics_rf = best_threshold_f1(yva, va_prob_rf)\n",
    "te_prob_rf = rf.predict_proba(Xte_flat)[:, 1]\n",
    "te_metrics_rf = compute_metrics(yte, te_prob_rf, threshold=best_t_rf)\n",
    "\n",
    "baseline_results[\"RandomForest\"] = {\n",
    "    \"val_prob\": va_prob_rf, \"test_prob\": te_prob_rf,\n",
    "    \"best_t\": best_t_rf,\n",
    "    \"val\": va_metrics_rf, \"test\": te_metrics_rf\n",
    "}\n",
    "\n",
    "print(\"RF best threshold (val):\", best_t_rf, \"| val metrics:\", va_metrics_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid CNN-RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)         \n",
    "        self.y = torch.from_numpy(y).float() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 128  \n",
    "train_loader = DataLoader(SeqDataset(Xtr_seq, ytr_seq), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(SeqDataset(Xva_seq, yva_seq), batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(SeqDataset(Xte_seq, yte_seq), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FraudCNNRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN over the time dimension (sequence of transactions),\n",
    "    followed by an RNN (LSTM) to capture longer-range dependencies.\n",
    "\n",
    "    Input:  x  shape (B, T, F)\n",
    "    Output: logits shape (B,)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int,\n",
    "        conv_channels: int = 64,\n",
    "        kernel_size: int = 3,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN expects (B, C, T). We'll treat features as channels (C=F).\n",
    "        # Convolution runs across time T.\n",
    "        padding = kernel_size // 2  # keeps T same when stride=1\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(num_features, conv_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "        # LSTM will take (B, T, conv_channels)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F) -> (B, F, T)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # CNN across time: (B, F, T) -> (B, C, T)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # back to RNN format: (B, C, T) -> (B, T, C)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # LSTM: use final hidden state\n",
    "        _, (h_n, _) = self.rnn(x)\n",
    "        h_last = h_n[-1]          # (B, hidden_size)\n",
    "\n",
    "        logits = self.fc(h_last).squeeze(1)  # (B,)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        return self.fc(h_n[-1]).squeeze(1)  # logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = FraudLSTM(num_features=Xtr_seq.shape[2], hidden_size=64).to(device)\n",
    "model = FraudCNNRNN(\n",
    "    num_features=Xtr_seq.shape[2],\n",
    "    conv_channels=64,\n",
    "    kernel_size=3,\n",
    "    hidden_size=64,\n",
    "    num_layers=1,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "pos = ytr_seq.sum()\n",
    "neg = len(ytr_seq) - pos\n",
    "raw_pw = neg / max(pos, 1)\n",
    "# pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float32).to(device)\n",
    "pos_weight = torch.tensor([min(raw_pw, 50.0)], dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | loss=0.3329 | val_pr_auc=0.002606\n",
      "Epoch 2 | loss=0.2851 | val_pr_auc=0.003836\n",
      "Epoch 3 | loss=0.2611 | val_pr_auc=0.003550\n",
      "Epoch 4 | loss=0.2683 | val_pr_auc=0.002362\n",
      "Epoch 5 | loss=0.2546 | val_pr_auc=0.003928\n",
      "Epoch 6 | loss=0.2447 | val_pr_auc=0.002223\n",
      "Epoch 7 | loss=0.2350 | val_pr_auc=0.005006\n",
      "Epoch 8 | loss=0.2181 | val_pr_auc=0.003970\n",
      "Epoch 9 | loss=0.2167 | val_pr_auc=0.005005\n",
      "Epoch 10 | loss=0.2044 | val_pr_auc=0.007647\n",
      "Epoch 11 | loss=0.1975 | val_pr_auc=0.002831\n",
      "Epoch 12 | loss=0.1823 | val_pr_auc=0.004729\n",
      "Epoch 13 | loss=0.1735 | val_pr_auc=0.003916\n",
      "Epoch 14 | loss=0.1807 | val_pr_auc=0.002774\n",
      "Epoch 15 | loss=0.1658 | val_pr_auc=0.003305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "def eval_model(model, loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_p.append(probs)\n",
    "            all_y.append(yb.numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_y).astype(int)\n",
    "    y_prob = np.concatenate(all_p)\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    fpr = ((y_pred == 1) & (y_true == 0)).sum() / max((y_true == 0).sum(), 1)\n",
    "\n",
    "    roc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    pr_auc = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"fpr\": fpr, \"roc_auc\": roc, \"pr_auc\": pr_auc}\n",
    "\n",
    "\n",
    "def eval_pr_auc(model, loader):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_p.append(probs)\n",
    "            all_y.append(yb.numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_y).astype(int)\n",
    "    y_prob = np.concatenate(all_p)\n",
    "    return average_precision_score(y_true, y_prob)\n",
    "\n",
    "def train_epochs(model, epochs=5):\n",
    "    best_pr, best_state = -1, None\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(yb)\n",
    "\n",
    "        val_pr = eval_pr_auc(model, val_loader)\n",
    "        print(f\"Epoch {epoch} | loss={total_loss/len(train_loader.dataset):.4f} | val_pr_auc={val_pr:.6f}\")\n",
    "\n",
    "        if val_pr > best_pr:\n",
    "            best_pr = val_pr\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "train_epochs(model, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>best_threshold_from_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>fpr</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN-RNN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.560401</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.527183</td>\n",
       "      <td>0.003913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-RNN</td>\n",
       "      <td>val</td>\n",
       "      <td>0.560401</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.592731</td>\n",
       "      <td>0.007647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>test</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>0.002514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>val</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.027761</td>\n",
       "      <td>0.500616</td>\n",
       "      <td>0.001261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>test</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.066052</td>\n",
       "      <td>0.521660</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>val</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.070537</td>\n",
       "      <td>0.537346</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model split  best_threshold_from_val  precision    recall        f1  \\\n",
       "5       CNN-RNN  test                 0.560401   0.019608  0.111111  0.033333   \n",
       "4       CNN-RNN   val                 0.560401   0.013333  0.133333  0.024242   \n",
       "1        LogReg  test                 0.990000   0.004338  0.074074  0.008197   \n",
       "0        LogReg   val                 0.990000   0.002398  0.066667  0.004630   \n",
       "3  RandomForest  test                 0.010000   0.002018  0.074074  0.003929   \n",
       "2  RandomForest   val                 0.010000   0.000945  0.066667  0.001864   \n",
       "\n",
       "        fpr   roc_auc    pr_auc  \n",
       "5  0.010018  0.527183  0.003913  \n",
       "4  0.009877  0.592731  0.007647  \n",
       "1  0.030655  0.515080  0.002514  \n",
       "0  0.027761  0.500616  0.001261  \n",
       "3  0.066052  0.521660  0.002157  \n",
       "2  0.070537  0.537346  0.001190  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "def get_probs_from_loader(model, loader, device):\n",
    "    model.eval()\n",
    "    all_y, all_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_prob.append(prob)\n",
    "            all_y.append(yb.numpy())\n",
    "    return np.concatenate(all_y).astype(int), np.concatenate(all_prob)\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    fpr = ((y_pred == 1) & (y_true == 0)).sum() / max((y_true == 0).sum(), 1)\n",
    "    roc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    pr  = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"fpr\": fpr, \"roc_auc\": roc, \"pr_auc\": pr}\n",
    "\n",
    "def best_threshold_f1(y_true, y_prob, thresholds=None, min_pred_pos=1):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "\n",
    "    if thresholds is None:\n",
    "        qs = np.linspace(0.01, 0.99, 300)\n",
    "        thresholds = np.unique(np.quantile(y_prob, qs))\n",
    "\n",
    "    best_t, best_f1, best_m = 0.5, -1, None\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        if y_pred.sum() < min_pred_pos:\n",
    "            continue  # skip thresholds that predict nothing positive\n",
    "\n",
    "        m = compute_metrics(y_true, y_prob, threshold=float(t))\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1, best_t, best_m = m[\"f1\"], float(t), m\n",
    "\n",
    "    # fallback: if everything got skipped, return 0.5\n",
    "    if best_m is None:\n",
    "        best_t, best_m = 0.5, compute_metrics(y_true, y_prob, threshold=0.5)\n",
    "\n",
    "    return best_t, best_m\n",
    "\n",
    "# ---- LSTM metrics ----\n",
    "\n",
    "# yva_true_lstm, yva_prob_lstm = get_probs_from_loader(model, val_loader, device)\n",
    "# best_t_lstm, va_metrics_lstm = best_threshold_f1(yva_true_lstm, yva_prob_lstm)\n",
    "yva_true_cnn, yva_prob_cnn = get_probs_from_loader(model, val_loader, device)\n",
    "best_t_cnn, va_metrics_cnn = best_threshold_f1(yva_true_cnn, yva_prob_cnn)\n",
    "\n",
    "# yte_true_lstm, yte_prob_lstm = get_probs_from_loader(model, test_loader, device)\n",
    "# te_metrics_lstm = compute_metrics(yte_true_lstm, yte_prob_lstm, threshold=best_t_lstm)\n",
    "yte_true_cnn, yte_prob_cnn = get_probs_from_loader(model, test_loader, device)\n",
    "te_metrics_cnn = compute_metrics(yte_true_cnn, yte_prob_cnn, threshold=best_t_cnn)\n",
    "\n",
    "# ---- Build comparison table ----\n",
    "rows = []\n",
    "\n",
    "def add_row(name, best_t, split, metrics):\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"split\": split,\n",
    "        \"best_threshold_from_val\": best_t,\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "# baselines (from Cell A)\n",
    "for name, info in baseline_results.items():\n",
    "    add_row(name, info[\"best_t\"], \"val\",  info[\"val\"])\n",
    "    add_row(name, info[\"best_t\"], \"test\", info[\"test\"])\n",
    "\n",
    "# LSTM\n",
    "# add_row(\"LSTM\", best_t_lstm, \"val\",  va_metrics_lstm)\n",
    "# add_row(\"LSTM\", best_t_lstm, \"test\", te_metrics_lstm)\n",
    "\n",
    "add_row(\"CNN-RNN\", best_t_cnn, \"val\",  va_metrics_cnn)\n",
    "add_row(\"CNN-RNN\", best_t_cnn, \"test\", te_metrics_cnn)\n",
    "\n",
    "df_compare = pd.DataFrame(rows)\n",
    "\n",
    "# nicer ordering\n",
    "df_compare = df_compare[[\n",
    "    \"model\",\"split\",\"best_threshold_from_val\",\n",
    "    \"precision\",\"recall\",\"f1\",\"fpr\",\"roc_auc\",\"pr_auc\"\n",
    "]].sort_values([\"model\",\"split\"])\n",
    "\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fraud rate: 0.0018 count: 27 / 15000\n",
      "val fraud rate: 0.001 count: 15 / 15000\n",
      "test fraud rate: 0.0018 count: 27 / 15000\n",
      "clients with >=1 fraud: 296 / 300\n"
     ]
    }
   ],
   "source": [
    "print(\"train fraud rate:\", ytr_seq.mean(), \"count:\", ytr_seq.sum(), \"/\", len(ytr_seq))\n",
    "print(\"val fraud rate:\", yva_seq.mean(), \"count:\", yva_seq.sum(), \"/\", len(yva_seq))\n",
    "print(\"test fraud rate:\", yte_seq.mean(), \"count:\", yte_seq.sum(), \"/\", len(yte_seq))\n",
    "# after merging labels into tx\n",
    "fraud_by_client = tx.groupby(\"client_id\")[\"target\"].sum(min_count=1)\n",
    "print(\"clients with >=1 fraud:\", (fraud_by_client > 0).sum(), \"/\", fraud_by_client.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val prob min/mean/max: 0.0053295097 0.06495062 0.6725309\n",
      "0.01 0.9025333333333333\n",
      "0.05 0.24466666666666667\n",
      "0.1 0.1496\n",
      "0.2 0.0898\n",
      "0.5 0.021533333333333335\n"
     ]
    }
   ],
   "source": [
    "# 1) Are probabilities all near a constant?\n",
    "yva_true, yva_prob = get_probs_from_loader(model, val_loader, device)\n",
    "print(\"val prob min/mean/max:\", yva_prob.min(), yva_prob.mean(), yva_prob.max())\n",
    "\n",
    "# 2) How many predicted positives at a few thresholds?\n",
    "for t in [0.01, 0.05, 0.1, 0.2, 0.5]:\n",
    "    print(t, (yva_prob >= t).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# delete big arrays + loaders + model\n",
    "del Xtr_seq, ytr_seq, Xva_seq, yva_seq, Xte_seq, yte_seq\n",
    "del train_loader, val_loader, test_loader\n",
    "del model, optimizer, criterion\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# if on GPU:\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
