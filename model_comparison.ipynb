{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q kagglehub\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "DATASET_ID = \"computingvictor/transactions-fraud-datasets\"\n",
    "DATASET_FOLDER_NAME = \"transactions-fraud-datasets\"\n",
    "\n",
    "N_CLIENTS = 300\n",
    "SEQ_LEN = 10\n",
    "MAX_WINDOWS_TRAIN = 50_000\n",
    "MAX_WINDOWS_VAL   = 20_000\n",
    "MAX_WINDOWS_TEST  = 20_000\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "\n",
    "POS_WEIGHT_CAP = 50.0\n",
    "SEED = 0\n",
    "# =====================\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "def find_repo_root():\n",
    "    cur = Path.cwd().resolve()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "def resolve_dataset_dir():\n",
    "    env = os.getenv(\"FRAUD_DATA_DIR\")\n",
    "    if env:\n",
    "        p = Path(env).expanduser().resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    repo_root = find_repo_root()\n",
    "    local = repo_root / \"data\" / DATASET_FOLDER_NAME\n",
    "    if local.exists():\n",
    "        return local.resolve()\n",
    "\n",
    "    print(\"Dataset not found locally - downloading via kagglehub...\")\n",
    "    return Path(kagglehub.dataset_download(DATASET_ID)).resolve()\n",
    "\n",
    "def parse_amount(x):\n",
    "    x = str(x).replace(\"$\",\"\").replace(\",\",\"\").strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found locally - downloading via kagglehub...\n",
      "Using dataset directory: /Users/anissasoungpanya/.cache/kagglehub/datasets/computingvictor/transactions-fraud-datasets/versions/1\n",
      "Rows: 3358392\n",
      "Labeled rows: 2249957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>client_id</th>\n",
       "      <th>card_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>use_chip</th>\n",
       "      <th>merchant_state</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7475328</td>\n",
       "      <td>2010-01-01 00:02:00</td>\n",
       "      <td>561</td>\n",
       "      <td>4575</td>\n",
       "      <td>$14.57</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>IA</td>\n",
       "      <td>5311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7475329</td>\n",
       "      <td>2010-01-01 00:02:00</td>\n",
       "      <td>1129</td>\n",
       "      <td>102</td>\n",
       "      <td>$80.00</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>CA</td>\n",
       "      <td>4829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7475333</td>\n",
       "      <td>2010-01-01 00:07:00</td>\n",
       "      <td>1807</td>\n",
       "      <td>165</td>\n",
       "      <td>$4.81</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>NY</td>\n",
       "      <td>5942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7475337</td>\n",
       "      <td>2010-01-01 00:21:00</td>\n",
       "      <td>351</td>\n",
       "      <td>1112</td>\n",
       "      <td>$10.74</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>NY</td>\n",
       "      <td>5813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7475344</td>\n",
       "      <td>2010-01-01 00:32:00</td>\n",
       "      <td>646</td>\n",
       "      <td>2093</td>\n",
       "      <td>$73.79</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>PA</td>\n",
       "      <td>7538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                 date  client_id  card_id  amount  \\\n",
       "0  7475328  2010-01-01 00:02:00        561     4575  $14.57   \n",
       "1  7475329  2010-01-01 00:02:00       1129      102  $80.00   \n",
       "2  7475333  2010-01-01 00:07:00       1807      165   $4.81   \n",
       "3  7475337  2010-01-01 00:21:00        351     1112  $10.74   \n",
       "4  7475344  2010-01-01 00:32:00        646     2093  $73.79   \n",
       "\n",
       "            use_chip merchant_state   mcc errors  target  \n",
       "0  Swipe Transaction             IA  5311    NaN     0.0  \n",
       "1  Swipe Transaction             CA  4829    NaN     0.0  \n",
       "2  Swipe Transaction             NY  5942    NaN     0.0  \n",
       "3  Swipe Transaction             NY  5813    NaN     NaN  \n",
       "4  Swipe Transaction             PA  7538    NaN     0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Labels + Transactions Subset\n",
    "\n",
    "dataset_dir = resolve_dataset_dir()\n",
    "print(\"Using dataset directory:\", dataset_dir)\n",
    "\n",
    "tx_path = dataset_dir / \"transactions_data.csv\"\n",
    "labels_path = dataset_dir / \"train_fraud_labels.json\"\n",
    "\n",
    "assert tx_path.exists(), f\"Missing {tx_path}\"\n",
    "assert labels_path.exists(), f\"Missing {labels_path}\"\n",
    "\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels_raw = json.load(f)\n",
    "\n",
    "target_map = labels_raw[\"target\"]\n",
    "labels = pd.DataFrame({\n",
    "    \"transaction_id\": list(target_map.keys()),\n",
    "    \"target\": [1 if v == \"Yes\" else 0 for v in target_map.values()]\n",
    "})\n",
    "labels[\"transaction_id\"] = labels[\"transaction_id\"].astype(str)\n",
    "labels[\"target\"] = labels[\"target\"].astype(int)\n",
    "\n",
    "usecols = [\"id\",\"date\",\"client_id\",\"card_id\",\"amount\",\"use_chip\",\"merchant_state\",\"mcc\",\"errors\"]\n",
    "\n",
    "# choose clients from an early slice\n",
    "first_chunk = pd.read_csv(tx_path, usecols=[\"client_id\"], nrows=200_000)\n",
    "clients = first_chunk[\"client_id\"].dropna().unique()\n",
    "\n",
    "chosen_clients = set(np.random.choice(clients, size=min(N_CLIENTS, len(clients)), replace=False))\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(tx_path, usecols=usecols, chunksize=200_000):\n",
    "    keep = chunk[chunk[\"client_id\"].isin(chosen_clients)]\n",
    "    if len(keep):\n",
    "        chunks.append(keep)\n",
    "\n",
    "tx = pd.concat(chunks, ignore_index=True)\n",
    "tx[\"id\"] = tx[\"id\"].astype(str)\n",
    "\n",
    "tx = tx.merge(labels, left_on=\"id\", right_on=\"transaction_id\", how=\"left\").drop(columns=[\"transaction_id\"])\n",
    "\n",
    "print(\"Rows:\", len(tx))\n",
    "print(\"Labeled rows:\", tx[\"target\"].notna().sum())\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>client_id</th>\n",
       "      <th>card_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>use_chip</th>\n",
       "      <th>merchant_state</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>target</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>delta_t_client_sec</th>\n",
       "      <th>delta_t_card_sec</th>\n",
       "      <th>log_delta_t_client</th>\n",
       "      <th>log_delta_t_card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7477094</td>\n",
       "      <td>2010-01-01 11:58:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>15.09</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>FL</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>13140.0</td>\n",
       "      <td>41640.0</td>\n",
       "      <td>9.483492</td>\n",
       "      <td>10.636841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7477168</td>\n",
       "      <td>2010-01-01 12:11:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3682</td>\n",
       "      <td>6.01</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>FL</td>\n",
       "      <td>5813</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>41640.0</td>\n",
       "      <td>6.660575</td>\n",
       "      <td>10.636841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7477216</td>\n",
       "      <td>2010-01-01 12:18:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3682</td>\n",
       "      <td>14.58</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>420.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>6.042633</td>\n",
       "      <td>6.042633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7477978</td>\n",
       "      <td>2010-01-01 15:09:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>14.66</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>10260.0</td>\n",
       "      <td>11460.0</td>\n",
       "      <td>9.236106</td>\n",
       "      <td>9.346705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7478279</td>\n",
       "      <td>2010-01-01 16:26:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>22.77</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>FL</td>\n",
       "      <td>4121</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4620.0</td>\n",
       "      <td>4620.0</td>\n",
       "      <td>8.438366</td>\n",
       "      <td>8.438366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                date  client_id  card_id  amount  \\\n",
       "0  7477094 2010-01-01 11:58:00          1     4652   15.09   \n",
       "1  7477168 2010-01-01 12:11:00          1     3682    6.01   \n",
       "2  7477216 2010-01-01 12:18:00          1     3682   14.58   \n",
       "3  7477978 2010-01-01 15:09:00          1     4652   14.66   \n",
       "4  7478279 2010-01-01 16:26:00          1     4652   22.77   \n",
       "\n",
       "             use_chip merchant_state   mcc errors  target  hour  dayofweek  \\\n",
       "0   Swipe Transaction             FL  4121   None     0.0    11          4   \n",
       "1   Swipe Transaction             FL  5813   None     0.0    12          4   \n",
       "2  Online Transaction        Unknown  4121   None     0.0    12          4   \n",
       "3  Online Transaction        Unknown  4121   None     0.0    15          4   \n",
       "4   Swipe Transaction             FL  4121   None     NaN    16          4   \n",
       "\n",
       "   delta_t_client_sec  delta_t_card_sec  log_delta_t_client  log_delta_t_card  \n",
       "0             13140.0           41640.0            9.483492         10.636841  \n",
       "1               780.0           41640.0            6.660575         10.636841  \n",
       "2               420.0             420.0            6.042633          6.042633  \n",
       "3             10260.0           11460.0            9.236106          9.346705  \n",
       "4              4620.0            4620.0            8.438366          8.438366  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean + Basic Feature Columns\n",
    "\n",
    "tx[\"date\"] = pd.to_datetime(tx[\"date\"], errors=\"coerce\")\n",
    "tx = tx.dropna(subset=[\"date\"])\n",
    "\n",
    "tx[\"amount\"] = tx[\"amount\"].apply(parse_amount)\n",
    "tx = tx.dropna(subset=[\"amount\"])\n",
    "\n",
    "tx[\"errors\"] = tx[\"errors\"].fillna(\"None\").astype(str)\n",
    "tx[\"use_chip\"] = tx[\"use_chip\"].fillna(\"Unknown\").astype(str)\n",
    "tx[\"merchant_state\"] = tx[\"merchant_state\"].fillna(\"Unknown\").astype(str)\n",
    "tx[\"mcc\"] = tx[\"mcc\"].fillna(-1).astype(int).astype(str)\n",
    "\n",
    "tx = tx.sort_values([\"client_id\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "tx[\"hour\"] = tx[\"date\"].dt.hour\n",
    "tx[\"dayofweek\"] = tx[\"date\"].dt.dayofweek\n",
    "\n",
    "# time-delta / \"velocity\" features (using only past info)\n",
    "tx[\"delta_t_client_sec\"] = tx.groupby(\"client_id\")[\"date\"].diff().dt.total_seconds()\n",
    "tx[\"delta_t_card_sec\"]   = tx.groupby(\"card_id\")[\"date\"].diff().dt.total_seconds()\n",
    "\n",
    "# fill missing deltas (first transaction per group) with median delta\n",
    "for col in [\"delta_t_client_sec\", \"delta_t_card_sec\"]:\n",
    "    tx[col] = tx[col].fillna(tx[col].median())\n",
    "    # cap extreme gaps to reduce outlier impact\n",
    "    cap = tx[col].quantile(0.99)\n",
    "    tx[col] = tx[col].clip(lower=0, upper=cap)\n",
    "\n",
    "# log transform to compress scale (more stable for models)\n",
    "tx[\"log_delta_t_client\"] = np.log1p(tx[\"delta_t_client_sec\"])\n",
    "tx[\"log_delta_t_card\"]   = np.log1p(tx[\"delta_t_card_sec\"])\n",
    "\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350739 503758 503895\n"
     ]
    }
   ],
   "source": [
    "def time_split(df, client_col=\"client_id\", frac_train=0.70, frac_val=0.15, min_len=20):\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for cid, g in df.groupby(client_col, sort=False):\n",
    "        n = len(g)\n",
    "        if n < min_len:\n",
    "            continue\n",
    "        t1 = int(n * frac_train)\n",
    "        t2 = int(n * (frac_train + frac_val))\n",
    "        idx = g.index.to_numpy()\n",
    "        train_idx.append(idx[:t1])\n",
    "        val_idx.append(idx[t1:t2])\n",
    "        test_idx.append(idx[t2:])\n",
    "    return np.concatenate(train_idx), np.concatenate(val_idx), np.concatenate(test_idx)\n",
    "\n",
    "train_idx, val_idx, test_idx = time_split(tx)\n",
    "\n",
    "train_df = tx.loc[train_idx].copy()\n",
    "val_df   = tx.loc[val_idx].copy()\n",
    "test_df  = tx.loc[test_idx].copy()\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2350739, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode + Scale\n",
    "\n",
    "num_cols = [\"amount\", \"hour\", \"dayofweek\", \"log_delta_t_client\", \"log_delta_t_card\"]\n",
    "cat_cols = [\"use_chip\", \"merchant_state\", \"mcc\", \"errors\"]\n",
    "\n",
    "enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Xtr_cat = enc.fit_transform(train_df[cat_cols])\n",
    "Xva_cat = enc.transform(val_df[cat_cols])\n",
    "Xte_cat = enc.transform(test_df[cat_cols])\n",
    "\n",
    "Xtr_num = scaler.fit_transform(train_df[num_cols])\n",
    "Xva_num = scaler.transform(val_df[num_cols])\n",
    "Xte_num = scaler.transform(test_df[num_cols])\n",
    "\n",
    "X_train = np.hstack([Xtr_num, Xtr_cat]).astype(np.float32)\n",
    "X_val   = np.hstack([Xva_num, Xva_cat]).astype(np.float32)\n",
    "X_test  = np.hstack([Xte_num, Xte_cat]).astype(np.float32)\n",
    "\n",
    "y_train = train_df[\"target\"].to_numpy()\n",
    "y_val   = val_df[\"target\"].to_numpy()\n",
    "y_test  = test_df[\"target\"].to_numpy()\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train windows: (15000, 10, 9) fraud rate: 0.0018 count: 27\n",
      "Val windows: (15000, 10, 9) fraud rate: 0.001 count: 15\n",
      "Test windows: (15000, 10, 9) fraud rate: 0.0018 count: 27\n"
     ]
    }
   ],
   "source": [
    "def build_sequences_sampled(df_part, X_part, y_part, seq_len=10, max_windows=50_000, client_col=\"client_id\", per_client_cap=50, seed=0):\n",
    "    X_seqs = np.zeros((max_windows, seq_len, X_part.shape[1]), dtype=np.float32)\n",
    "    y_seqs = np.zeros((max_windows,), dtype=np.int64)\n",
    "\n",
    "    k = 0\n",
    "    start = 0\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for cid, g in df_part.groupby(client_col, sort=False):\n",
    "        n = len(g)\n",
    "        if n <= seq_len:\n",
    "            start += n\n",
    "            continue\n",
    "\n",
    "        Xg = X_part[start:start+n]\n",
    "        yg = y_part[start:start+n]\n",
    "\n",
    "        valid_t = np.where(~np.isnan(yg))[0]\n",
    "        valid_t = valid_t[valid_t >= seq_len]\n",
    "        if len(valid_t) == 0:\n",
    "            start += n\n",
    "            continue\n",
    "\n",
    "        take = min(len(valid_t), per_client_cap)\n",
    "        chosen = rng.choice(valid_t, size=take, replace=False)\n",
    "\n",
    "        for t in chosen:\n",
    "            if k >= max_windows:\n",
    "                return X_seqs[:k], y_seqs[:k]\n",
    "            X_seqs[k] = Xg[t-seq_len:t]\n",
    "            y_seqs[k] = int(yg[t])\n",
    "            k += 1\n",
    "\n",
    "        start += n\n",
    "\n",
    "    return X_seqs[:k], y_seqs[:k]\n",
    "\n",
    "\n",
    "Xtr_seq, ytr_seq = build_sequences_sampled(train_df, X_train, y_train, seq_len=SEQ_LEN, max_windows=MAX_WINDOWS_TRAIN, seed=SEED)\n",
    "Xva_seq, yva_seq = build_sequences_sampled(val_df,   X_val,   y_val,   seq_len=SEQ_LEN, max_windows=MAX_WINDOWS_VAL, seed=SEED)\n",
    "Xte_seq, yte_seq = build_sequences_sampled(test_df,  X_test,  y_test,  seq_len=SEQ_LEN, max_windows=MAX_WINDOWS_TEST, seed=SEED)\n",
    "\n",
    "print(\"Train windows:\", Xtr_seq.shape, \"fraud rate:\", ytr_seq.mean(), \"count:\", int(ytr_seq.sum()))\n",
    "print(\"Val windows:\",   Xva_seq.shape, \"fraud rate:\", yva_seq.mean(), \"count:\", int(yva_seq.sum()))\n",
    "print(\"Test windows:\",  Xte_seq.shape, \"fraud rate:\", yte_seq.mean(), \"count:\", int(yte_seq.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    fpr = ((y_pred == 1) & (y_true == 0)).sum() / max((y_true == 0).sum(), 1)\n",
    "\n",
    "    roc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    pr  = average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"fpr\": fpr, \"roc_auc\": roc, \"pr_auc\": pr}\n",
    "\n",
    "def best_threshold_f1(y_true, y_prob, thresholds=None, min_pred_pos=1):\n",
    "    \"\"\"\n",
    "    Picks threshold that maximizes F1 on validation, but avoids degenerate thresholds\n",
    "    that predict 0 positives (or fewer than min_pred_pos).\n",
    "\n",
    "    Uses quantiles of y_prob as candidate thresholds by default (more stable than fixed linspace).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "\n",
    "    if thresholds is None:\n",
    "        qs = np.linspace(0.01, 0.99, 300)\n",
    "        thresholds = np.unique(np.quantile(y_prob, qs))\n",
    "\n",
    "    best_t, best_f1, best_m = 0.5, -1, None\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_prob >= t).astype(int)\n",
    "        if y_pred.sum() < min_pred_pos:\n",
    "            continue\n",
    "        m = compute_metrics(y_true, y_prob, threshold=float(t))\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1, best_t, best_m = m[\"f1\"], float(t), m\n",
    "\n",
    "    if best_m is None:\n",
    "        best_t, best_m = 0.5, compute_metrics(y_true, y_prob, threshold=0.5)\n",
    "\n",
    "    return best_t, best_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg: {'best_t': 0.9989585424887892, 'val': {'precision': 0.006666666666666667, 'recall': 0.06666666666666667, 'f1': 0.012121212121212121, 'fpr': np.float64(0.009943276609943277), 'roc_auc': 0.5033611389166945, 'pr_auc': 0.002445907147746244}, 'test': {'precision': 0.006060606060606061, 'recall': 0.037037037037037035, 'f1': 0.010416666666666666, 'fpr': np.float64(0.010953048821211514), 'roc_auc': 0.5628699560443365, 'pr_auc': 0.007005624797213152}}\n",
      "RandomForest: {'best_t': 0.0033271111111111114, 'val': {'precision': 0.0014099400775467042, 'recall': 0.5333333333333333, 'f1': 0.0028124450694322375, 'fpr': np.float64(0.3781114447781114), 'roc_auc': 0.5333044155266378, 'pr_auc': 0.0010992016488155852}, 'test': {'precision': 0.0010783608914450035, 'recall': 0.2222222222222222, 'f1': 0.0021463065641209086, 'fpr': np.float64(0.37120149602618047), 'roc_auc': 0.43351489471171567, 'pr_auc': 0.001741175698951547}}\n"
     ]
    }
   ],
   "source": [
    "# Sklearn Baselines\n",
    "\n",
    "Xtr_flat = Xtr_seq.reshape(Xtr_seq.shape[0], -1)\n",
    "Xva_flat = Xva_seq.reshape(Xva_seq.shape[0], -1)\n",
    "Xte_flat = Xte_seq.reshape(Xte_seq.shape[0], -1)\n",
    "\n",
    "ytr = ytr_seq.astype(np.int64)\n",
    "yva = yva_seq.astype(np.int64)\n",
    "yte = yte_seq.astype(np.int64)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\n",
    "logreg.fit(Xtr_flat, ytr)\n",
    "\n",
    "va_prob_lr = logreg.predict_proba(Xva_flat)[:, 1]\n",
    "best_t_lr, va_metrics_lr = best_threshold_f1(yva, va_prob_lr)\n",
    "te_prob_lr = logreg.predict_proba(Xte_flat)[:, 1]\n",
    "te_metrics_lr = compute_metrics(yte, te_prob_lr, threshold=best_t_lr)\n",
    "\n",
    "baseline_results[\"LogReg\"] = {\"best_t\": best_t_lr, \"val\": va_metrics_lr, \"test\": te_metrics_lr}\n",
    "print(\"LogReg:\", baseline_results[\"LogReg\"])\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=300, min_samples_leaf=2, class_weight=\"balanced_subsample\", n_jobs=-1, random_state=SEED)\n",
    "rf.fit(Xtr_flat, ytr)\n",
    "\n",
    "va_prob_rf = rf.predict_proba(Xva_flat)[:, 1]\n",
    "best_t_rf, va_metrics_rf = best_threshold_f1(yva, va_prob_rf)\n",
    "te_prob_rf = rf.predict_proba(Xte_flat)[:, 1]\n",
    "te_metrics_rf = compute_metrics(yte, te_prob_rf, threshold=best_t_rf)\n",
    "\n",
    "baseline_results[\"RandomForest\"] = {\"best_t\": best_t_rf, \"val\": va_metrics_rf, \"test\": te_metrics_rf}\n",
    "print(\"RandomForest:\", baseline_results[\"RandomForest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Dataset + Loaders\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(SeqDataset(Xtr_seq, ytr_seq), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(SeqDataset(Xva_seq, yva_seq), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(SeqDataset(Xte_seq, yte_seq), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions (CNN, CNN-RNN)\n",
    "\n",
    "class FraudCNNRNN(nn.Module):\n",
    "    def __init__(self, num_features, conv_channels=64, kernel_size=3, hidden_size=64, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(num_features, conv_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)      # (B,T,F)->(B,F,T)\n",
    "        x = self.conv(x)           # (B,C,T)\n",
    "        x = x.transpose(1, 2)      # (B,T,C)\n",
    "        _, (h_n, _) = self.rnn(x)\n",
    "        h_last = h_n[-1]\n",
    "        return self.fc(h_last).squeeze(1)\n",
    "\n",
    "class FraudCNNBaseline(nn.Module):\n",
    "    def __init__(self, num_features, conv_channels=64, kernel_size=3, dropout=0.1, pool=\"max\"):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(num_features, conv_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        assert pool in (\"max\", \"avg\")\n",
    "        self.pool = pool\n",
    "        self.fc = nn.Linear(conv_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)      # (B,T,F)->(B,F,T)\n",
    "        x = self.conv(x)           # (B,C,T)\n",
    "        x = x.max(dim=2).values if self.pool == \"max\" else x.mean(dim=2)  # (B,C)\n",
    "        return self.fc(x).squeeze(1)\n",
    "\n",
    "class FraudLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=64, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)     # h_n: (num_layers, B, hidden)\n",
    "        h_last = h_n[-1]               # (B, hidden)\n",
    "        return self.fc(h_last).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Torch Train/Eval Functions\n",
    "\n",
    "def get_pos_weight(y_np, cap=POS_WEIGHT_CAP):\n",
    "    pos = float(np.sum(y_np))\n",
    "    neg = float(len(y_np) - pos)\n",
    "    raw = neg / max(pos, 1.0)\n",
    "    return torch.tensor([min(raw, cap)], dtype=torch.float32, device=device)\n",
    "\n",
    "def eval_pr_auc_torch(model, loader):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_p.append(prob)\n",
    "            all_y.append(yb.numpy())\n",
    "    y_true = np.concatenate(all_y).astype(int)\n",
    "    y_prob = np.concatenate(all_p)\n",
    "    return average_precision_score(y_true, y_prob)\n",
    "\n",
    "def get_probs_from_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            all_p.append(prob)\n",
    "            all_y.append(yb.numpy())\n",
    "    return np.concatenate(all_y).astype(int), np.concatenate(all_p)\n",
    "\n",
    "def train_torch_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n",
    "    model = model.to(device)\n",
    "    pos_weight = get_pos_weight(ytr_seq, cap=POS_WEIGHT_CAP)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_pr, best_state = -1, None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(yb)\n",
    "\n",
    "        val_pr = eval_pr_auc_torch(model, val_loader)\n",
    "        print(f\"Epoch {epoch:02d} | loss={total_loss/len(train_loader.dataset):.4f} | val_pr_auc={val_pr:.6f}\")\n",
    "\n",
    "        if val_pr > best_pr:\n",
    "            best_pr = val_pr\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.3233 | val_pr_auc=0.001026\n",
      "Epoch 02 | loss=0.2875 | val_pr_auc=0.002045\n",
      "Epoch 03 | loss=0.2645 | val_pr_auc=0.003059\n",
      "Epoch 04 | loss=0.2542 | val_pr_auc=0.001861\n",
      "Epoch 05 | loss=0.2511 | val_pr_auc=0.004133\n",
      "Epoch 06 | loss=0.2377 | val_pr_auc=0.003004\n",
      "Epoch 07 | loss=0.2155 | val_pr_auc=0.003726\n",
      "Epoch 08 | loss=0.2169 | val_pr_auc=0.001819\n",
      "Epoch 09 | loss=0.1910 | val_pr_auc=0.002116\n",
      "Epoch 10 | loss=0.2020 | val_pr_auc=0.004246\n",
      "Epoch 11 | loss=0.1816 | val_pr_auc=0.004045\n",
      "Epoch 12 | loss=0.1751 | val_pr_auc=0.002523\n",
      "Epoch 13 | loss=0.1472 | val_pr_auc=0.002417\n",
      "Epoch 14 | loss=0.1730 | val_pr_auc=0.001934\n",
      "Epoch 15 | loss=0.1537 | val_pr_auc=0.001762\n"
     ]
    }
   ],
   "source": [
    "# Train CNN-RNN\n",
    "\n",
    "cnn_rnn = FraudCNNRNN(num_features=Xtr_seq.shape[2], conv_channels=64, kernel_size=3, hidden_size=64, num_layers=1, dropout=0.1)\n",
    "cnn_rnn = train_torch_model(cnn_rnn, train_loader, val_loader, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.4033 | val_pr_auc=0.001374\n",
      "Epoch 02 | loss=0.2873 | val_pr_auc=0.002542\n",
      "Epoch 03 | loss=0.2646 | val_pr_auc=0.002740\n",
      "Epoch 04 | loss=0.2532 | val_pr_auc=0.002118\n",
      "Epoch 05 | loss=0.2388 | val_pr_auc=0.002211\n",
      "Epoch 06 | loss=0.2211 | val_pr_auc=0.002051\n",
      "Epoch 07 | loss=0.2120 | val_pr_auc=0.001150\n",
      "Epoch 08 | loss=0.2286 | val_pr_auc=0.001166\n",
      "Epoch 09 | loss=0.2021 | val_pr_auc=0.001108\n",
      "Epoch 10 | loss=0.1868 | val_pr_auc=0.001518\n",
      "Epoch 11 | loss=0.1724 | val_pr_auc=0.001189\n",
      "Epoch 12 | loss=0.1560 | val_pr_auc=0.001752\n",
      "Epoch 13 | loss=0.1470 | val_pr_auc=0.000948\n",
      "Epoch 14 | loss=0.1388 | val_pr_auc=0.001902\n",
      "Epoch 15 | loss=0.1436 | val_pr_auc=0.001241\n"
     ]
    }
   ],
   "source": [
    "# Train CNN Baseline\n",
    "\n",
    "cnn = FraudCNNBaseline(num_features=Xtr_seq.shape[2], conv_channels=64, kernel_size=3, dropout=0.1, pool=\"max\")\n",
    "cnn = train_torch_model(cnn, train_loader, val_loader, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.3581 | val_pr_auc=0.001171\n",
      "Epoch 02 | loss=0.2802 | val_pr_auc=0.001274\n",
      "Epoch 03 | loss=0.2779 | val_pr_auc=0.006333\n",
      "Epoch 04 | loss=0.2752 | val_pr_auc=0.011796\n",
      "Epoch 05 | loss=0.2809 | val_pr_auc=0.007345\n",
      "Epoch 06 | loss=0.2733 | val_pr_auc=0.005837\n",
      "Epoch 07 | loss=0.2732 | val_pr_auc=0.008158\n",
      "Epoch 08 | loss=0.2691 | val_pr_auc=0.003481\n",
      "Epoch 09 | loss=0.2645 | val_pr_auc=0.016433\n",
      "Epoch 10 | loss=0.2642 | val_pr_auc=0.004963\n",
      "Epoch 11 | loss=0.2632 | val_pr_auc=0.012358\n",
      "Epoch 12 | loss=0.2549 | val_pr_auc=0.002657\n",
      "Epoch 13 | loss=0.2542 | val_pr_auc=0.004350\n",
      "Epoch 14 | loss=0.2585 | val_pr_auc=0.003084\n",
      "Epoch 15 | loss=0.2513 | val_pr_auc=0.002932\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM\n",
    "\n",
    "lstm = FraudLSTM(num_features=Xtr_seq.shape[2], hidden_size=64, num_layers=1, dropout=0.1)\n",
    "lstm = train_torch_model(lstm, train_loader, val_loader, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>best_threshold_from_val</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>fpr</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.114856</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.091698</td>\n",
       "      <td>0.609114</td>\n",
       "      <td>0.003040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN</td>\n",
       "      <td>val</td>\n",
       "      <td>0.114856</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.091692</td>\n",
       "      <td>0.685063</td>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN-RNN</td>\n",
       "      <td>test</td>\n",
       "      <td>0.467768</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.019635</td>\n",
       "      <td>0.530352</td>\n",
       "      <td>0.004215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-RNN</td>\n",
       "      <td>val</td>\n",
       "      <td>0.467768</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.659148</td>\n",
       "      <td>0.004246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>test</td>\n",
       "      <td>0.297354</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.014092</td>\n",
       "      <td>0.426899</td>\n",
       "      <td>0.002579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>val</td>\n",
       "      <td>0.297354</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>0.546894</td>\n",
       "      <td>0.016433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>0.562870</td>\n",
       "      <td>0.007006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>val</td>\n",
       "      <td>0.998959</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.009943</td>\n",
       "      <td>0.503361</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>test</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.371201</td>\n",
       "      <td>0.433515</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>val</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.378111</td>\n",
       "      <td>0.533304</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model split  best_threshold_from_val  precision    recall        f1  \\\n",
       "7           CNN  test                 0.114856   0.003628  0.185185  0.007117   \n",
       "6           CNN   val                 0.114856   0.004348  0.400000  0.008602   \n",
       "5       CNN-RNN  test                 0.467768   0.010101  0.111111  0.018519   \n",
       "4       CNN-RNN   val                 0.467768   0.010067  0.200000  0.019169   \n",
       "9          LSTM  test                 0.297354   0.009390  0.074074  0.016667   \n",
       "8          LSTM   val                 0.297354   0.015000  0.200000  0.027907   \n",
       "1        LogReg  test                 0.998959   0.006061  0.037037  0.010417   \n",
       "0        LogReg   val                 0.998959   0.006667  0.066667  0.012121   \n",
       "3  RandomForest  test                 0.003327   0.001078  0.222222  0.002146   \n",
       "2  RandomForest   val                 0.003327   0.001410  0.533333  0.002812   \n",
       "\n",
       "        fpr   roc_auc    pr_auc  \n",
       "7  0.091698  0.609114  0.003040  \n",
       "6  0.091692  0.685063  0.002740  \n",
       "5  0.019635  0.530352  0.004215  \n",
       "4  0.019686  0.659148  0.004246  \n",
       "9  0.014092  0.426899  0.002579  \n",
       "8  0.013146  0.546894  0.016433  \n",
       "1  0.010953  0.562870  0.007006  \n",
       "0  0.009943  0.503361  0.002446  \n",
       "3  0.371201  0.433515  0.001741  \n",
       "2  0.378111  0.533304  0.001099  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate All Models + Build Final Table\n",
    "\n",
    "rows = []\n",
    "\n",
    "def add_row(name, split, best_t, metrics):\n",
    "    rows.append({\"model\": name, \"split\": split, \"best_threshold_from_val\": best_t, **metrics})\n",
    "\n",
    "# sklearn baselines\n",
    "for name, info in baseline_results.items():\n",
    "    add_row(name, \"val\",  info[\"best_t\"], info[\"val\"])\n",
    "    add_row(name, \"test\", info[\"best_t\"], info[\"test\"])\n",
    "\n",
    "# CNN-RNN\n",
    "yva_true, yva_prob = get_probs_from_loader(cnn_rnn, val_loader)\n",
    "best_t, va_m = best_threshold_f1(yva_true, yva_prob)\n",
    "yte_true, yte_prob = get_probs_from_loader(cnn_rnn, test_loader)\n",
    "te_m = compute_metrics(yte_true, yte_prob, threshold=best_t)\n",
    "add_row(\"CNN-RNN\", \"val\", best_t, va_m)\n",
    "add_row(\"CNN-RNN\", \"test\", best_t, te_m)\n",
    "\n",
    "# CNN\n",
    "yva_true, yva_prob = get_probs_from_loader(cnn, val_loader)\n",
    "best_t, va_m = best_threshold_f1(yva_true, yva_prob)\n",
    "yte_true, yte_prob = get_probs_from_loader(cnn, test_loader)\n",
    "te_m = compute_metrics(yte_true, yte_prob, threshold=best_t)\n",
    "add_row(\"CNN\", \"val\", best_t, va_m)\n",
    "add_row(\"CNN\", \"test\", best_t, te_m)\n",
    "\n",
    "# LSTM\n",
    "yva_true, yva_prob = get_probs_from_loader(lstm, val_loader)\n",
    "best_t, va_m = best_threshold_f1(yva_true, yva_prob)\n",
    "yte_true, yte_prob = get_probs_from_loader(lstm, test_loader)\n",
    "te_m = compute_metrics(yte_true, yte_prob, threshold=best_t)\n",
    "add_row(\"LSTM\", \"val\", best_t, va_m)\n",
    "add_row(\"LSTM\", \"test\", best_t, te_m)\n",
    "\n",
    "df_compare = pd.DataFrame(rows)\n",
    "df_compare = df_compare[[\n",
    "    \"model\",\"split\",\"best_threshold_from_val\",\n",
    "    \"precision\",\"recall\",\"f1\",\"fpr\",\"roc_auc\",\"pr_auc\"\n",
    "]].sort_values([\"model\",\"split\"])\n",
    "\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fraud rate: 0.0018 count: 27 / 15000\n",
      "val fraud rate: 0.001 count: 15 / 15000\n",
      "test fraud rate: 0.0018 count: 27 / 15000\n",
      "clients with >=1 fraud: 296 / 300\n",
      "CNN-RNN val prob min/mean/max: 0.0037344834 0.06968447 0.74603146\n",
      "CNN val prob min/mean/max: 0.008381785 0.055328917 0.59090626\n"
     ]
    }
   ],
   "source": [
    "print(\"train fraud rate:\", ytr_seq.mean(), \"count:\", int(ytr_seq.sum()), \"/\", len(ytr_seq))\n",
    "print(\"val fraud rate:\",   yva_seq.mean(), \"count:\", int(yva_seq.sum()), \"/\", len(yva_seq))\n",
    "print(\"test fraud rate:\",  yte_seq.mean(), \"count:\", int(yte_seq.sum()), \"/\", len(yte_seq))\n",
    "\n",
    "fraud_by_client = tx.groupby(\"client_id\")[\"target\"].sum(min_count=1)\n",
    "print(\"clients with >=1 fraud:\", int((fraud_by_client > 0).sum()), \"/\", fraud_by_client.shape[0])\n",
    "\n",
    "# probability ranges\n",
    "yva_true_rnn, yva_prob_rnn = get_probs_from_loader(cnn_rnn, val_loader)\n",
    "print(\"CNN-RNN val prob min/mean/max:\", yva_prob_rnn.min(), yva_prob_rnn.mean(), yva_prob_rnn.max())\n",
    "\n",
    "yva_true_cnn, yva_prob_cnn = get_probs_from_loader(cnn, val_loader)\n",
    "print(\"CNN val prob min/mean/max:\", yva_prob_cnn.min(), yva_prob_cnn.mean(), yva_prob_cnn.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "\n",
    "del Xtr_seq, Xva_seq, Xte_seq\n",
    "del ytr_seq, yva_seq, yte_seq\n",
    "del train_loader, val_loader, test_loader\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
